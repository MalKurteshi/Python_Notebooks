{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 - Data Oriented Programming Paradigms - Group 20\n",
    "\n",
    "## Team Members: Schöbinger-Hassve Sebastian, Kurteshi Mal, Kaçuri Muhamet, Ademi Ard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic: What are typical characteristics for a large flow of refugees? Are there typical characteristics of large flows of immigration? Can refugee flows between two countries be predicted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pycountry as pc\n",
    "import math\n",
    "import seaborn as sns\n",
    "from IPython.core.display import display, HTML\n",
    "from sklearn.neighbors import NearestNeighbors, KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, BayesianRidge, SGDRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which dataset(s) did you choose? Why?\n",
    "\n",
    "The dataset picked in this exercise are the once listed below: \n",
    "- UN List of Refugees + Venezuelans displaced abroad (Link: https://www.unhcr.org/refugee-statistics/download/?url=L6zuyL)\n",
    "- Worldwide Governance Indicators (https://datacatalog.worldbank.org/dataset/worldwide-governance-indicators)\n",
    "- GDP(Gross domestic product) for Countries throw years (https://databank.worldbank.org/reports.aspx?source=2&series=NY.GDP.MKTP.CD&country=# ) \n",
    "- UCDP One-sided Violence Dataset version 20.1 (https://ucdp.uu.se/downloads/index.html#onesided) \n",
    "- Death by natural disasters (https://ourworldindata.org/grapher/natural-disaster-deaths-ihme?tab=table)\n",
    "- Conflicts \n",
    "- 10 Distances (https://www.geodatos.net/en/distances/countries/)\n",
    "\n",
    "We have choosen these datasets in order to be able to obtain as good insights as possible for our aim to answer these questions. As the base dataset we have chosen the UN list of refugees and we have added additional information to this dataset by merging the data from other datasets the we thought would be helpful. We think that some factors like governance indicators, GDP, violence, conflicts and natural disastres can have an impact to the immigration flow, mainly this is the reason why we have chosen to enrich our base dataset with this additional information. Another information that will be helpful in our further analysis is the distance between the country of origin and the country of asylum, since this information is absent in the UN list of refugees we have chosesn to merge this information from another dataset that contains these distances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How did you clean/transform the data? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our merged dataset contains a lot of missing information since we have added information from different sources. In the cases where the information was missing and we couldn’t jus impute these missing values like for example in the most obvious case there are years that we simply don’t have information for a lot of features that we added from different sources.\n",
    "First, we had to chose a time span between 1996 and 2018 since not all datasets covered the same years. We did some manual correction of the data like adding some missing ISO values. Another cleaning decision that we made was to remove stateless refugees since we thought that this information would not be useful in the answers that we want to get from the data. We noticed that some country names needed to be transformed since they included ‘,’ and this messed with csv format. We also did some manual imputation on the distances between countries since some distances weren’t covered in the distance dataset.\n",
    "The GDP values are joined with the \"Control of Corruption\", \"Rule of Law\" estimators into one dataset. Once the overall dataset is created, missing values are dealt with: 1. in case of a few missing columns, they are imputed with the estimated difference of previous/later values 2, if the row is entirely empty it is filled up with 0 and an additional column is set to 1, indicating that the values of the statistics attributes were NaNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_df = pd.read_csv('./1_data_acquisition/dataset_03/gdp.csv', sep=',')\n",
    "distance_df = pd.read_csv('./1_data_acquisition/dataset_10.csv', sep=',')\n",
    "refugee_df = pd.read_csv('./1_data_acquisition/dataset_01/fixed_population.csv', sep=',')\n",
    "statistics_df = pd.read_csv('./1_data_acquisition/dataset_02/WGIData.csv', sep=',')\n",
    "natural_death_df = pd.read_csv('./1_data_acquisition/dataset_07/natural-disaster-deaths-ihme.csv', sep=',')\n",
    "conflict_df = pd.read_csv('./1_data_acquisition/dataset_06/ucdp-onesided-201.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce dataset to usable years\n",
    "Since not all datasets cover the same time period, we settled for a time span between 1996 and 2018. We chose against covering also 2019, since multiple countries lacked their GDP for 2019 in the responsible dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 1996\n",
    "end_year = 2018\n",
    "\n",
    "refugee_df = refugee_df[(refugee_df['Year'] >= start_year) & (refugee_df['Year'] <= end_year)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data manipulation \n",
    "\n",
    "In the section below we are going to define functions that will help us for the data tranformation in features like distance in between coutries. This action is nesscesary because of the need for a defined distance in between country of origin and the country of destination. The distance is presented in miles and from the functions below is made uniform for one merged dataframe in future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_short(entry):\n",
    "    val = pc.countries.get(name=entry['Country Name'])\n",
    "    if not val:\n",
    "        val = pc.countries.get(common_name=entry['Country Name'])\n",
    "        \n",
    "    if not val:\n",
    "        try:\n",
    "            vals = pc.countries.search_fuzzy(entry['Country Name'])\n",
    "            if len(vals) == 1:\n",
    "                val = vals[0]\n",
    "        except:\n",
    "            return \"\"\n",
    "    \n",
    "    if val:\n",
    "        return val.alpha_3\n",
    "    else:\n",
    "        return \"\"    \n",
    "    \n",
    "def add_distance(entry):\n",
    "    src_country = entry['Country of origin (ISO)']\n",
    "    dst_country = entry['Country of asylum (ISO)']\n",
    "    \n",
    "    src_name = country_lookup[country_lookup['ISO'] == src_country]['Country Name'].values[0]\n",
    "    dst_name = country_lookup[country_lookup['ISO'] == dst_country]['Country Name'].values[0]\n",
    "    \n",
    "    distance_entry = distance_df[(distance_df['Source Country Norm'] == src_name) &\n",
    "                                 (distance_df['Destination Country Norm'] == dst_name)]\n",
    "    \n",
    "    dist_val = distance_entry['Shortest distance'].values\n",
    "    \n",
    "    if len(dist_val) == 0 or np.isnan(dist_val):\n",
    "        dist_val = distance_entry['Shortest distance between major cities'].values\n",
    "        \n",
    "    if len(dist_val) == 0 or np.isnan(dist_val):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return dist_val[0]\n",
    "    \n",
    "def update_iso(country_name, iso):\n",
    "    ix =country_lookup[country_lookup['Country Name'] == country_name].index\n",
    "    country_lookup.loc[ix,'ISO'] = iso\n",
    "    \n",
    "def update_country(src, dst, val):\n",
    "    ix = distance_df[(distance_df['Source Country Norm']== src) &\n",
    "                     (distance_df['Destination Country Norm']== dst)].index\n",
    "    distance_df.loc[ix, 'Shortest distance'] = val\n",
    "    \n",
    "    ix = distance_df[(distance_df['Source Country Norm']== dst) & \n",
    "                     (distance_df['Destination Country Norm']== src)].index\n",
    "    distance_df.loc[ix, 'Shortest distance'] = val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Country lookup Table\n",
    "\n",
    "Create a dataframe, from which we can lookup the Name and corresponding ISO 3166-1 alpha-3 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_lookup = pd.DataFrame(columns=['Country Name', 'ISO'])\n",
    "country_lookup['Country Name'] = np.unique(distance_df['Source Country Norm'].values)\n",
    "country_lookup['ISO'] = country_lookup.apply(lambda x: get_short(x), axis=1)\n",
    "update_iso(\"Kosovo\", \"XKX\")\n",
    "update_iso(\"Curacao\", \"CUW\")\n",
    "update_iso(\"Iran\", \"IRN\")\n",
    "update_iso(\"South Korea\", \"KOR\")\n",
    "update_iso(\"Bonaire, Saint Eustatius and Saba \", \"BES\")\n",
    "update_iso(\"Cocos Islands\", \"CCK\")\n",
    "update_iso(\"Democratic Republic of the Congo\", \"COD\")\n",
    "update_iso(\"Laos\", \"LAO\")\n",
    "update_iso(\"Netherlands Antilles\", \"NLD\")\n",
    "update_iso(\"Palestinian Territory\", \"PSE\")\n",
    "update_iso(\"Timor Leste\", \"TLS\")\n",
    "update_iso(\"U.S. Virgin Islands\", \"VIR\")\n",
    "update_iso(\"North Korea\", \"PRK\")\n",
    "update_iso(\"Ivory Coast\", \"CIV\")\n",
    "update_iso(\"Reunion\", \"REU\")\n",
    "update_iso(\"Sint Maarten\", \"SXM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Refugee Dataset\n",
    "\n",
    "* Remove stateless refugees\n",
    "* Add \"Venezuelans abroad\" to the refugee counter\n",
    "* Manually modify country ISO connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add venezuelan refugees to overall count\n",
    "refugee_df['Venezuelans displaced abroad'].fillna(0, inplace=True)\n",
    "refugee_df['Refugees under UNHCR’s mandate'] = refugee_df['Refugees under UNHCR’s mandate'] + refugee_df['Venezuelans displaced abroad']\n",
    "refugee_df.drop(['Venezuelans displaced abroad'], inplace=True, axis=1)\n",
    "\n",
    "\n",
    "# Drop all refugees where we don't know where they come from\n",
    "refugee_df.dropna(subset=['Country of origin (ISO)', 'Country of asylum (ISO)'], inplace=True)\n",
    "refugee_df = refugee_df[refugee_df['Country of origin (ISO)'] != \"XXA\"]\n",
    "refugee_df = refugee_df[refugee_df['Country of asylum (ISO)'] != \"XXA\"]\n",
    "\n",
    "# Make Tibet part of China\n",
    "ix = refugee_df[refugee_df['Country of origin (ISO)'] == \"TIB\"].index\n",
    "refugee_df.loc[ix,'Country of origin (ISO)'] = \"CHN\"\n",
    "\n",
    "ix = refugee_df[refugee_df['Country of asylum (ISO)'] == \"TIB\"].index\n",
    "refugee_df.loc[ix,'Country of asylum (ISO)'] = \"CHN\"\n",
    "\n",
    "# Remove , which mess with the csv format\n",
    "refugee_df['Country of origin'] = refugee_df['Country of origin'].str.replace(',', '.')\n",
    "refugee_df['Country of asylum'] = refugee_df['Country of asylum'].str.replace(',', '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Distances to refugee Dataset\n",
    "\n",
    "Manually add distances and add the distance between the country of origin and country of asylum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting Distances\n"
     ]
    }
   ],
   "source": [
    "update_country('China', 'India', 0)\n",
    "update_country('Venezuela', 'Curacao', 0)\n",
    "update_country('Venezuela', 'Aruba', 0)\n",
    "update_country('Liberia', 'Nigeria', 1138)\n",
    "update_country('Rwanda', 'Burundi', 0)\n",
    "update_country('Tunisia', 'Austria', 1052)\n",
    "update_country('Nicaragua', 'Spain', 8376)\n",
    "update_country('Sudan', 'Liberia', 3328)\n",
    "update_country('Iran', 'Spain', 3445)\n",
    "update_country('Kyrgyzstan', 'Spain', 5433)\n",
    "update_country('Belarus', 'Sweden', 911)\n",
    "update_country('Nicaragua', 'Spain', 8376)\n",
    "update_country('Sudan', 'Liberia', 3328)\n",
    "update_country('Iran', 'Spain', 3445)\n",
    "update_country('Kyrgyzstan', 'Spain', 5433)\n",
    "update_country('Belarus', 'Sweden', 911)\n",
    "update_country('Nicaragua', 'Belgium', 8424)\n",
    "update_country('Nicaragua', 'Italy', 8825)\n",
    "update_country('Benin', 'Netherlands', 4288)\n",
    "update_country('Bangladesh', 'Trinidad and Tobago', 14730)\n",
    "update_country('Andorra', 'Germany', 1170)\n",
    "update_country('Niger', 'United Kingdom', 3187)\n",
    "update_country('Andorra', 'United States', 7864)\n",
    "update_country('Eritrea', 'Bosnia and Herzegovina', 3812)\n",
    "update_country('Niger', 'Germany', 1170)\n",
    "update_country('Jordan', 'Hungary', 1963)\n",
    "update_country('Malaysia', 'United Kingdom', 9999)\n",
    "update_country('Sudan', 'Iceland', 5538)\n",
    "\n",
    "print('Inserting Distances')\n",
    "refugee_df.insert(refugee_df.shape[1], 'Distance', refugee_df.apply(lambda x: add_distance(x), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling empty entries ('..') from the original csv file\n",
    "def gdp_fix(entry, country):\n",
    "    \n",
    "    year = entry['Year']\n",
    "    \n",
    "    src = entry['{} (ISO)'.format(country)]\n",
    "    \n",
    "    gdp_entry = gdp_df[(gdp_df['Country Code'] == src)]\n",
    "    gdp = gdp_entry['{} [YR{}]'.format(year, year)].values\n",
    "    \n",
    "    if len(gdp_entry) == 0 or gdp[0] == \"..\":\n",
    "        return np.nan\n",
    "            \n",
    "    return gdp[0]\n",
    "\n",
    "def entry_hole_plugger(entry, min_year, max_year):\n",
    "\n",
    "    first_year = -1\n",
    "    first_val = -1\n",
    "    last_year = -1\n",
    "    last_val = -1\n",
    "    empty_years = []\n",
    "    \n",
    "    for i in range(min_year, max_year):\n",
    "        field_name = '{}'.format(i)\n",
    "            \n",
    "        if field_name in entry:\n",
    "            year_value = entry[field_name]\n",
    "        else:\n",
    "            year_value = ''\n",
    "        if year_value == '' or year_value == '..':\n",
    "            if first_year != -1:\n",
    "                empty_years.append(i)    \n",
    "            continue  \n",
    "        if len(empty_years) == 0:\n",
    "            first_year = i\n",
    "            first_val = float(year_value)\n",
    "        else:\n",
    "            last_year = i\n",
    "            last_val = float(year_value)\n",
    "            \n",
    "        # Fill up empty years between two known years        \n",
    "        if first_year+1 < last_year: \n",
    "            val_diff = last_val - first_val  \n",
    "            years_diff = last_year - first_year             \n",
    "            yearly_diff = val_diff/years_diff \n",
    "            for empty_year in empty_years:\n",
    "                now_years = empty_year - first_year \n",
    "                now_val = first_val + (now_years * yearly_diff) \n",
    "                now_val = round(now_val, 5)                \n",
    "                field_name = '{}'.format(empty_year)                    \n",
    "                entry[field_name] = str(now_val)\n",
    "                \n",
    "            first_year = i\n",
    "            first_val = float(year_value)\n",
    "            empty_years = []\n",
    "            \n",
    "    # Fill up empty first or last years\n",
    "    for i in range(min_year, max_year):\n",
    "        field_name = '{}'.format(i)\n",
    "            \n",
    "        if field_name in entry:\n",
    "            year_value = entry[field_name]\n",
    "        else:\n",
    "            year_value = ''\n",
    "            \n",
    "                \n",
    "        if year_value == '' or year_value == '..':\n",
    "            if first_year == -1:\n",
    "                empty_years.append(i)\n",
    "            \n",
    "            continue\n",
    "            \n",
    "        first_year = last_year\n",
    "        first_val = last_val\n",
    "        \n",
    "        last_year = i\n",
    "        last_val = float(year_value)\n",
    "                  \n",
    "        if last_year == first_year + 1 and len(empty_years) > 0:\n",
    "            val_diff = last_val - first_val\n",
    "            for empty_year in empty_years:\n",
    "                now_years = first_year - empty_year \n",
    "                now_val = first_val + (now_years * val_diff) \n",
    "                now_val = round(now_val, 5)\n",
    "                field_name = '{}'.format(empty_year)         \n",
    "                entry[field_name] = str(now_val)\n",
    "                \n",
    "            empty_years = []\n",
    "            \n",
    "    \n",
    "    if len(empty_years) > 0:\n",
    "        val_diff = last_val - first_val\n",
    "        for empty_year in empty_years:\n",
    "            now_years = first_year - empty_year \n",
    "            now_val = first_val + (now_years * val_diff) \n",
    "            now_val = round(now_val, 5)\n",
    "            field_name = '{}'.format(empty_year)         \n",
    "            entry[field_name] = str(now_val)\n",
    "    \n",
    "    entry = entry.sort_index()\n",
    "                    \n",
    "    return entry\n",
    "\n",
    "def add_stats_column(entry, columns, country):\n",
    "    year = entry['Year']\n",
    "    src = entry['{} (ISO)'.format(country)]\n",
    "    \n",
    "    stat_entry = overall_df[overall_df['Country'] == src][year]\n",
    "    values = stat_entry.values.tolist()\n",
    "\n",
    "    if len(values) != 7:\n",
    "        print(src)\n",
    "        print(stat_entry)\n",
    "        values = values[:7]\n",
    "        print(len(values))\n",
    "    \n",
    "    return values\n",
    "\n",
    "def add_natural_death(entry):\n",
    "    natural_death_entry = natural_death_df[(natural_death_df['Year'] == entry['Year']) &\n",
    "                                           (natural_death_df['Code'] == entry['Country of origin (ISO)'])]\n",
    "    if len(natural_death_entry) == 1:\n",
    "        value = natural_death_entry['Deaths - Exposure to forces of nature - Sex: Both - Age: All Ages (Number)'].values\n",
    "        return value[0]\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def add_value_is_na(entry, columns):\n",
    "    nas = entry[columns].isna().any()\n",
    "    if nas:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Country Statistics\n",
    "\n",
    "The GDP values are joined with the \"Control of Corruption\", \"Rule of Law\", .. estimators into one dataset.\n",
    "Once the overall dataset is created, missing values are dealt with:\n",
    "1. in case of a few missing columns, they are imputed with the estimated difference of previous/later values\n",
    "2. if the row is entirely empty, it is filled up with 0 and an additional column is set to 1, indicating that the values of the statistics attributes were NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_year_columns = {}\n",
    "all_year_columns = [str(i) for i in range(1960, 2019)]\n",
    "stats_year_columns = [str(i) for i in range(start_year, end_year+1)]\n",
    "\n",
    "for i in range(1960, 2021):\n",
    "    gdp_year_columns['{} [YR{}]'.format(i, i)] = str(i)\n",
    "    \n",
    "gdp_df = gdp_df.rename(gdp_year_columns, axis=1)    \n",
    "gdp_df[all_year_columns] = gdp_df[all_year_columns].apply(lambda x: entry_hole_plugger(x, 1960, 2019), axis=1)\n",
    "\n",
    "statistics_columns = [\"Control of Corruption: Estimate\", \n",
    "                      \"Government Effectiveness: Estimate\", \n",
    "                      \"Political Stability and Absence of Violence/Terrorism: Estimate\", \n",
    "                      \"Regulatory Quality: Estimate\",\n",
    "                      \"Rule of Law: Estimate\",\n",
    "                      \"Voice and Accountability: Estimate\"]\n",
    "\n",
    "overall_columns = ['GDP']\n",
    "overall_columns += statistics_columns        \n",
    "\n",
    "values = []\n",
    "countries = np.unique(list(country_lookup['ISO'].values))\n",
    "for country in countries:\n",
    "    # Add GPD Values\n",
    "    tmp_list = gdp_df[gdp_df['Country Code'] == country].get(stats_year_columns).values.tolist()\n",
    "    if len(tmp_list) == 0:\n",
    "        tmp_list = [np.nan for i in range(start_year, end_year+1)]\n",
    "    else:\n",
    "        tmp_list = tmp_list[0]\n",
    "        tmp_list = list(map(lambda x: np.nan if x == \"..\" or x == \"\" else x, tmp_list))\n",
    "        \n",
    "    tmp_list = [country, 'GDP'] + tmp_list\n",
    "        \n",
    "    values.append(tmp_list)\n",
    "    \n",
    "    country_df = statistics_df[statistics_df['Country Code'] == country]\n",
    "    \n",
    "    for column in statistics_columns:\n",
    "        tmp_df = country_df[country_df['Indicator Name'] == column].filter(items=stats_year_columns)\n",
    "        if len(tmp_df) == 0:\n",
    "            tmp_list = [np.nan for i in range(start_year, end_year+1)]\n",
    "        else:\n",
    "            tmp_df = tmp_df.apply(lambda x: entry_hole_plugger(x, start_year, end_year+1), axis=1)\n",
    "            tmp_list = tmp_df.get(stats_year_columns).values.tolist()[0]\n",
    "\n",
    "        tmp_list = [country, column] + tmp_list    \n",
    "        \n",
    "        values.append(tmp_list)\n",
    "    \n",
    "column_names = ['Country', 'Statistics']\n",
    "column_names +=  [i for i in range(start_year, end_year+1)]\n",
    "\n",
    "overall_df = pd.DataFrame(data=values, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"GDP\",\n",
    "           \"Control of Corruption: Estimate\", \n",
    "           \"Government Effectiveness: Estimate\", \n",
    "           \"Political Stability and Absence of Violence/Terrorism: Estimate\", \n",
    "           \"Regulatory Quality: Estimate\",\n",
    "           \"Rule of Law: Estimate\",\n",
    "           \"Voice and Accountability: Estimate\"]\n",
    "\n",
    "country_cols = {'Country of origin': '_origin', 'Country of asylum': '_asylum'}\n",
    "\n",
    "column_names = []\n",
    "\n",
    "for country, suffix in country_cols.items():\n",
    "    column_names = []\n",
    "    for column in columns: \n",
    "        if ':' in column:\n",
    "            column_name, _ = column.split(':', 1)\n",
    "        else:\n",
    "            column_name = column\n",
    "        \n",
    "        column_names.append('{}{}'.format(column_name, suffix))\n",
    "                \n",
    "    print('Working on {}'.format(suffix))\n",
    "    content = refugee_df.apply(lambda x: add_stats_column(x, columns, country), axis=1)\n",
    "    for i in range(0, 7):\n",
    "        val = []\n",
    "        for e in content:\n",
    "            val.append(e[i])\n",
    "            \n",
    "        refugee_df[column_names[i]] = val\n",
    "        \n",
    "    refugee_df['isna_GDP{}'.format(suffix)] = refugee_df.apply(lambda x: add_value_is_na(x, ['GDP{}'.format(suffix)]), axis=1)\n",
    "    refugee_df['isna_statistics{}'.format(suffix)] = refugee_df.apply(lambda x: add_value_is_na(x, column_names[1:]), axis=1)\n",
    "    \n",
    "refugee_df = refugee_df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add \"deaths due to nature\" to origin country\n",
    "\n",
    "The amount of deaths which were caused by natural incidents (storms, earthquake, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refugee_df.insert(refugee_df.shape[1], 'Natural Deaths in origin', refugee_df.apply(lambda x: add_natural_death(x), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add conflict status to origin country\n",
    "\n",
    "Add the conflict indicator to the country of origin. This indicates (civil) wars or other types of unrests which might have caused the refugees to flee\n",
    "\n",
    "The civil wars in Tijikistan and the Ukraine were additionally added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joinable(entry):\n",
    "    question = pair_conflict_df[(pair_conflict_df['Year'] == entry['Year']) &\n",
    "                               (pair_conflict_df['ISO'] == entry['Country of origin (ISO)'])]\n",
    "    if len(question) > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def show_conflict(entry):\n",
    "    \n",
    "    for country in entry['location'].split(','):\n",
    "        country = country.strip()\n",
    "        country = country.split('(', 1)[0].strip()\n",
    "        if country == \"DR Congo\":\n",
    "            country = \"Democratic Republic of the Congo\"\n",
    "        elif country == \"Congo\":\n",
    "            country = \"Republic of the Congo\"\n",
    "        elif country == \"Bosnia-Herzegovina\":\n",
    "            country = \"Bosnia\"\n",
    "        alpha = country_lookup[country_lookup['Country Name'] == country]['ISO'].values\n",
    "        if len(alpha) == 0:\n",
    "            try:\n",
    "                countries = pc.countries.search_fuzzy(country)\n",
    "                if len(countries) > 1:\n",
    "                    print(country)\n",
    "                    raise Exception\n",
    "                return entry['year'], countries[0].alpha_3\n",
    "            except:\n",
    "                print('We couldn\\'t find {}'.format(country))\n",
    "                return country\n",
    "        \n",
    "        return entry['year'], alpha[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conflict_data = conflict_df.apply(lambda x: show_conflict(x), axis=1)\n",
    "pair_conflict_df = pd.DataFrame.from_records(conflict_data, columns=['Year', 'ISO'])\n",
    "\n",
    "# Adding civil war in Tijikistan\n",
    "for year in range(1992, 1998):\n",
    "    pair_conflict_df = pair_conflict_df.append({'Year': year, 'ISO':'TJK'}, ignore_index=True)\n",
    "    \n",
    "for year in range(2014, 2019):\n",
    "    pair_conflict_df = pair_conflict_df.append({'Year': year, 'ISO':'UKR'}, ignore_index=True)\n",
    "\n",
    "refugee_df['Conflict in origin'] = refugee_df.apply(lambda x: joinable(x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Ratios and Save to file\n",
    "\n",
    "Check the final dataset for NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratio(my_df):\n",
    "    print(len(my_df))\n",
    "    print(round(my_df.isna().sum() / len(my_df) * 100, 2))\n",
    "    \n",
    "get_ratio(refugee_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save final dataset to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refugee_df.to_csv('./2_dataset/final_dataset_preprocesed_v6.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How did you solve the problem of missing values? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the data transformation we ended still with missing values in our final dataset. \n",
    "\n",
    "The first approach included using KNNImputer\n",
    "The reason why we thought this would be useful is basen on the way on how this algorithm works in which each sample’s missing values are imputed using the mean value from n_neighbors nearest neighbors found in the training set. We thought that these missing values would be inputted with values that would be at least close to real values.\n",
    "\n",
    "The second attempt following approach was used:\n",
    "\n",
    "(V = existing values, X = missing values)\n",
    "Case 1: V V V X X X V V\n",
    "Case 2: X X X V V V X X\n",
    "Case 3: X X X X X X X X\n",
    "\n",
    "For Case 1, the missing values X were filled up with the difference of two existing values next to the missing ones.\n",
    "For Case 2, the missing values X were filled up with the difference of the two consecutive values prior or after.\n",
    "For Case 3, the missing values X were replaced with 0 and an additional column was set to 1, indicating that this row contained NaN values which could not be replaced.\n",
    "\n",
    "See function \"entry_hole_plugger\" and \"add_value_is_na\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What questions did you ask of the data? Why were these good questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The questions asked of the data are:\n",
    "- 1. What characteristics cause the refugee flows?\n",
    "- 2. Do some events (i.e. war) initiate a higher amount of refugees than others (i.e. natural disasters, economic breakdown)?\n",
    "- 3. What is the connection between origin and destination country? \n",
    "\n",
    "These questions were in very interest to us because they lead important insights to us such as indication on why the migration happens, what can be the reasons, where the migration starts, is there any correlation of the origin contry and the neighbourhood countries. What are factors and indications that impact the number of refugees etc.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What were the answers to these questions? How did you obtain them? Do the answers make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1. What characteristics cause the refugee flows?\n",
    "\n",
    "The answer to this question is based on the relation of number of refugees with the GDP value of the origin contry of migration, with the controll of corruption level in the origin country and with goverment effectiveness of the origin contry. From the plots below we can see that these reasons and the number of refugees have a correlation, with the drop of GDP, drop of controll of corruption (the raise of corruption) and the government effectivenes on the original contry the number of refugees is big from these origin contries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('./2_dataset/final_dataset_preprocesed_v6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x=\"GDP_origin\", y=\"Refugees under UNHCR’s mandate\", data=dataset)\n",
    "plt.savefig(\"./4_data_analysing_visualization/question_1_visualisation/NumberOfRefugees_GDP_Origin.JPG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x=\"Control of Corruption_origin\", y=\"Refugees under UNHCR’s mandate\", data=dataset)\n",
    "plt.savefig(\"./4_data_analysing_visualization/question_1_visualisation/NumberOfRefugees_CorruptionInOrigin.JPG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x=\"Government Effectiveness_origin\", y=\"Refugees under UNHCR’s mandate\", data=dataset)\n",
    "plt.savefig(\"./4_data_analysing_visualization/question_1_visualisation/NumberOfRefugees_GovernmentEffectiveness.JPG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2. Do some events (i.e. war) initiate a higher amount of refugees than others (i.e. natural disasters, economic breakdown)?\n",
    "\n",
    "We found out that some events create a higher number of refugees than others, but we can have also exceptions. Conflicts like wars, civil wars, internal conflicts create extremely more refugees flow than other events. Indexes like control of corruption, government effectiveness are also triggering but they create a smaller number of refugees and are spread over years. Another trigger is the economical reason which in some cases causes an extreme refugee flow (ex. Venezuela).\n",
    "We can see from plots below that Syria in 2012 has an exponential increase in refugees flows because of the beginning of the war and Venezuela which has an extreme increase in refugees in the year 2018 even though there was no war, but the causes were economic reasons. We can see two similar increases in refugee flows, but the causes are different. The red bar indicates that country was in conflict in that year and the blue indicates the country was at peace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df = pd.read_csv('./2_dataset/final_dataset_preprocesed_v6.csv')\n",
    "df = df.fillna(0)\n",
    "df = df.round(2)\n",
    "\n",
    "# Get People on /100000\n",
    "# Information about country\n",
    "# Country Parameter(Corruption etc) min and max value indexes\n",
    "\n",
    "df_information = df[[\n",
    "    'GDP_origin',\n",
    "    'Control of Corruption_origin',\n",
    "    'Government Effectiveness_origin',\n",
    "    'Political Stability and Absence of Violence/Terrorism_origin',\n",
    "    'Regulatory Quality_origin',\n",
    "    'Rule of Law_origin',\n",
    "    'Voice and Accountability_origin'\n",
    "]]\n",
    "# GDP_origin: min: 1.233485e+07 , max: 2.058016e+13\n",
    "# Control of Corruption_origin: min: -1.87, max: 2.47\n",
    "# Government Effectiveness_origin: min: -2.48, max: 2.44\n",
    "# Political Stability and Absence of Violence/Terrorism_origin: min: -3.31, max: 1.76\n",
    "# Regulatory Quality_origin: min: -2.65, max: 2.26\n",
    "# Rule of Law_origin: min: -2.61, max: 2.10\n",
    "# Voice and Accountability_origin: min: -2.31 ,  max: 1.80       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group by data to Format for a country during Years\n",
    "countries = df.groupby([\"Country of origin\", \"Year\"]).agg(\n",
    "    {\n",
    "     'Refugees under UNHCR’s mandate': 'sum',\n",
    "     'Control of Corruption_origin':'min',\n",
    "     'Political Stability and Absence of Violence/Terrorism_origin':'min',\n",
    "     'Rule of Law_origin':'min',\n",
    "     'Voice and Accountability_origin':'min',\n",
    "     'Natural Deaths in origin': 'max',\n",
    "     'Conflict in origin': 'max',\n",
    "    })\n",
    "\n",
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group by data to Format for a country during Years\n",
    "countries_corr = df.groupby([\"Country of origin\", \"Year\"]).agg(\n",
    "    {\n",
    "     'Refugees under UNHCR’s mandate': 'sum',\n",
    "     'Control of Corruption_origin':'min',\n",
    "     'Political Stability and Absence of Violence/Terrorism_origin':'min',\n",
    "     'Rule of Law_origin':'min',\n",
    "     'Voice and Accountability_origin':'min',\n",
    "     'Natural Deaths in origin': 'max',\n",
    "     'Conflict in origin': 'max',\n",
    "     'GDP_origin': 'max' \n",
    "    })\n",
    "\n",
    "countries_corr\n",
    "\n",
    "# Mean attributes for each state during years\n",
    "mean_correlation = countries_corr.reset_index()\n",
    "mean_correlation.rename({'index': 'Country of origin'}, axis='columns', inplace=True)\n",
    "#Get each state mean for each paramaters throw years 1996-2018\n",
    "mean_correlation = countries_corr.groupby([\"Country of origin\"]).mean()\n",
    "mean_correlation.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeCountryData(c, showTable = False):\n",
    "    \n",
    "    print(c)\n",
    "    ctr =countries.loc[[c]]\n",
    "    \n",
    "    #   Display all Counntry data by years\n",
    "    if showTable:\n",
    "        countryData = ctr.T\n",
    "        display(HTML(countryData.to_html()))\n",
    "\n",
    "    \n",
    "    ctr = ctr.reset_index()\n",
    "    ctr.rename({'index': 'Country of origin'}, axis='columns', inplace=True)\n",
    "    \n",
    "#     display(HTML(ctr.corr().to_html()))\n",
    "\n",
    "    \n",
    "    colors = []\n",
    "    for value in ctr.iterrows():\n",
    "        if 1 == value[1]['Conflict in origin']:\n",
    "            colors.append('r')\n",
    "        else:\n",
    "            colors.append('b')\n",
    "    \n",
    "  \n",
    "    plt.bar(ctr['Year'], ctr['Refugees under UNHCR’s mandate'], align='center', alpha=0.5, color=colors)\n",
    "    plt.ylabel('Refugees')\n",
    "    plt.title(c)\n",
    "    \n",
    "    plt.xticks([1996,1999,2002,2005,2008,2011,2014,2018])\n",
    "    \n",
    "    \n",
    "# #   Corruption Index\n",
    "    minCorruptionIndex = ctr[ctr['Control of Corruption_origin'] == ctr['Control of Corruption_origin'].min()].Year\n",
    "    \n",
    "    if isinstance(minCorruptionIndex, pd.Series):\n",
    "        minCorruptionIndex = minCorruptionIndex.values[0]\n",
    "    \n",
    "    plt.vlines(minCorruptionIndex, 0, ctr['Refugees under UNHCR’s mandate'].max(), linestyles =\"dotted\", colors =\"r\")\n",
    "    plt.text(minCorruptionIndex,ctr['Refugees under UNHCR’s mandate'].max(),'Corruption: ' + str(ctr['Control of Corruption_origin'].min()))\n",
    "\n",
    "    #   Natural Deaths in origin\n",
    "    naturalDeathCases = ctr[ctr['Natural Deaths in origin'] == ctr['Natural Deaths in origin'].max()].Year\n",
    "\n",
    "    if isinstance(naturalDeathCases, pd.Series):\n",
    "        naturalDeathCases = naturalDeathCases.values[0] \n",
    "        \n",
    "    if  naturalDeathCases.max() != 0.0 or naturalDeathCases != 0.0:    \n",
    "        plt.vlines(naturalDeathCases, 0, ctr['Refugees under UNHCR’s mandate'].median(), linestyles =\"dotted\", colors =\"b\")\n",
    "        plt.text(naturalDeathCases,ctr['Refugees under UNHCR’s mandate'].median(),'Natural D.: ' + str(ctr['Natural Deaths in origin'].max()) )\n",
    " \n",
    "    \n",
    "    #   Political Stability and Absence of Violence/Terrorism_origin\n",
    "    ruleOfLawIndex = ctr[ctr['Rule of Law_origin'] == ctr['Rule of Law_origin'].min()].Year\n",
    "\n",
    "    if isinstance(ruleOfLawIndex, pd.Series):\n",
    "        ruleOfLawIndex = ruleOfLawIndex.values[0] \n",
    "        \n",
    "    plt.vlines(ruleOfLawIndex, 0, ctr['Refugees under UNHCR’s mandate'].mean(), linestyles =\"dotted\", colors =\"g\")\n",
    "    plt.text(ruleOfLawIndex,ctr['Refugees under UNHCR’s mandate'].mean() - 2,'R. of Law: ' + str(ctr['Rule of Law_origin'].min()))\n",
    "    \n",
    "    plt.savefig(\"./4_data_analysing_visualization/question_2_visualisation/\"+c+\".jpg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizeCountryData('Ukraine', True)\n",
    "visualizeCountryData('Venezuela (Bolivarian Republic of)', True)\n",
    "visualizeCountryData('Syrian Arab Rep.', True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3. What is the connection between origin and destination country?\n",
    "\n",
    "From the plots below we can see that the number of refugees presented taking into consideration the migration destinations but under the condition of the distance in between origin contry and asylum contry is bigger when the distance is small leading to conclusion that the number of migration from the destination country is quite bigger in the neighbourhood countries. Also a connection in made in this matter taking into consideration the number of refugees and the GDP giving us the conclusion that the migration hattpens in matter of better GDP countries.\n",
    "Both the cases where taken also for analysation just for the ballkan contries as a special case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_df = dataset[dataset[\"Country of origin\"] == \"Albania\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(y=\"Refugees under UNHCR’s mandate\", x=\"Distance\", hue=\"Country of origin\", style=\"Country of origin\",\n",
    "            kind=\"line\", data=k_df)\n",
    "plt.savefig(\"./4_data_analysing_visualization/question_3_visualisation/OriginAndDestinationDistance_Ballkan.JPG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(y=\"Refugees under UNHCR’s mandate\", x=\"Distance\", hue=\"Country of origin\",\n",
    "            kind=\"line\", data=dataset)\n",
    "plt.savefig(\"./4_data_analysing_visualization/question_3_visualisation/OriginAndDestinationDistance.JPG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(y=\"Refugees under UNHCR’s mandate\", x=\"GDP_asylum\", hue=\"Country of origin\", style=\"Country of origin\",\n",
    "            kind=\"line\", data=k_df)\n",
    "plt.savefig(\"./4_data_analysing_visualization/question_3_visualisation/OriginAndDestinationDistance_Ballkan_GDP.JPG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(y=\"Refugees under UNHCR’s mandate\", x=\"GDP_asylum\", hue=\"Country of origin\", style=\"Country of origin\",\n",
    "            kind=\"line\", data=dataset)\n",
    "plt.savefig(\"./4_data_analysing_visualization/question_3_visualisation/OriginAndDestinationDistance_Ballkan__All_GDP.JPG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Part\n",
    "\n",
    "Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refugee_df = pd.read_csv('./2_dataset/final_dataset_preprocesed_v6.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_df = refugee_df.drop(['Country of origin',\n",
    "                         'Country of origin (ISO)', \n",
    "                         'Country of asylum', \n",
    "                         'Country of asylum (ISO)'], axis=1)\n",
    "\n",
    "\n",
    "origin_cols = ['Control of Corruption_origin',\n",
    "               'Government Effectiveness_origin',\n",
    "               'Political Stability and Absence of Violence/Terrorism_origin',\n",
    "               'Regulatory Quality_origin', \n",
    "               'Rule of Law_origin',\n",
    "               'Voice and Accountability_origin']\n",
    "\n",
    "asylum_cols = ['Control of Corruption_asylum',\n",
    "               'Government Effectiveness_asylum',\n",
    "               'Political Stability and Absence of Violence/Terrorism_asylum',\n",
    "               'Regulatory Quality_asylum', \n",
    "               'Rule of Law_asylum',\n",
    "               'Voice and Accountability_asylum']\n",
    "\n",
    "for column in origin_cols:\n",
    "    for ix in ml_df[ml_df[column].isna()].index:\n",
    "        ml_df.loc[ix,'isna_statistics_origin'] = 1\n",
    "        ml_df.loc[ix, column] = 0\n",
    "        \n",
    "for column in asylum_cols:\n",
    "    for ix in ml_df[ml_df[column].isna()].index:\n",
    "        ml_df.loc[ix,'isna_statistics_asylum'] = 1\n",
    "        ml_df.loc[ix, column] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(16, 16))\n",
    "fig.tight_layout(pad=7.0)\n",
    "\n",
    "pearsoncorr = ml_df.corr(method='pearson')\n",
    "plot = sns.heatmap(pearsoncorr,\n",
    "            xticklabels=pearsoncorr.columns,\n",
    "            yticklabels=pearsoncorr.columns,\n",
    "            cmap='RdBu_r',\n",
    "            annot=True,\n",
    "            linewidth=0.5, \n",
    "            ax=ax)\n",
    "plot.set_xticklabels(plot.get_xticklabels(), rotation=20, horizontalalignment='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on the distance based on the origin country\n",
    "\n",
    "The aim of this prediction attempt is to evaluate, whether it is possible to predict the distance between the country of origin and the country of asylum.\n",
    "\n",
    "Based on the result scores, we deem the question to be NOT predictable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = ml_df.drop(['GDP_asylum', \n",
    "                     'Control of Corruption_asylum',\n",
    "                     'Government Effectiveness_asylum',       \n",
    "                     'Political Stability and Absence of Violence/Terrorism_asylum',\n",
    "                     'Regulatory Quality_asylum', \n",
    "                     'Rule of Law_asylum',\n",
    "                     'Voice and Accountability_asylum', \n",
    "                     'isna_GDP_asylum',\n",
    "                     'isna_statistics_asylum', \n",
    "                     'Natural Deaths in origin',\n",
    "                     'Conflict in origin'], axis=1)\n",
    "\n",
    "scalers = [\n",
    "    ('standard_scaler', StandardScaler())\n",
    "]\n",
    "models = [\n",
    "    ('Random Forest', RandomForestRegressor()),\n",
    "    ('ridge', RidgeCV()), \n",
    "    ('lasso', LassoCV()), \n",
    "    ('bayesian', BayesianRidge()),\n",
    "    ('nearest neighbor', KNeighborsRegressor(n_neighbors=4))\n",
    "]\n",
    "target_column = 'Distance'\n",
    "\n",
    "best_model = {}\n",
    "best_val = 0\n",
    "alphas = [0.2, 0.25, 0.3]\n",
    "\n",
    "for alpha in alphas:\n",
    "    for scaler_name, scaler in scalers:\n",
    "        for model_name, model in models:\n",
    "            \n",
    "            y = tmp_df.get([target_column])\n",
    "            X = tmp_df.drop([target_column], axis=1)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=alpha)\n",
    "\n",
    "            pipeline = Pipeline([\n",
    "                ('scaler', scaler),\n",
    "                ('clf', model)\n",
    "            ])\n",
    "\n",
    "            pipeline.fit(X_train, y_train.values.ravel())\n",
    "            if model_name == 'linearregression':\n",
    "                score = cross_val_score(pipeline, X_test, y_test, cv=10, scoring='r2')\n",
    "                score = abs(score.mean())\n",
    "            elif model_name == 'nearest neighbor':\n",
    "                scores = {}\n",
    "                for i in range(1, 6):\n",
    "                    model.n_neighbors = i\n",
    "                    pipeline = Pipeline([\n",
    "                        ('scaler', scaler),\n",
    "                        ('clf', model)\n",
    "                    ])\n",
    "                    \n",
    "                    pipeline.fit(X_train, y_train.values.ravel())\n",
    "                    \n",
    "                    score = pipeline.score(X_test, y_test)\n",
    "                    scores[score] = i\n",
    "                    \n",
    "                max_val = max(scores.keys())\n",
    "                model.n_neighbors = scores[max_val]\n",
    "                model_name += ' {}'.format(i)\n",
    "                scores = max_val\n",
    "            else:\n",
    "                score = pipeline.score(X_test, y_test)\n",
    "\n",
    "            if score > best_val:\n",
    "                best_model['model'] = ((model_name, model))\n",
    "                best_model['scaler'] = ((scaler_name, scaler))\n",
    "                best_model['alpha'] = alpha\n",
    "                best_val = score\n",
    "\n",
    "print('Best Model: {} -> ({},{},{})'.format(\n",
    "    best_val, \n",
    "    best_model['model'][0], \n",
    "    best_model['scaler'][0],\n",
    "    best_model['alpha']\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on the amount of refugees between the country of origin and asylum\n",
    "\n",
    "The aim of this prediction attempt was to determine whether ML algorithms are able to predict the amount of refugee which will be requesting asylum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = ml_df.copy()\n",
    "\n",
    "scalers = [\n",
    "    ('standard_scaler', StandardScaler())\n",
    "]\n",
    "models = [\n",
    "    ('ridge', RidgeCV()), \n",
    "    ('lasso', LassoCV()), \n",
    "    ('bayesian', BayesianRidge()),\n",
    "    ('nearest neighbor', KNeighborsRegressor(n_neighbors=4)),\n",
    "    ('Random Forest', RandomForestRegressor())\n",
    "]\n",
    "target_column = 'Refugees under UNHCR’s mandate'\n",
    "\n",
    "best_model = {}\n",
    "best_val = 0\n",
    "alphas = [0.2, 0.25, 0.3]\n",
    "\n",
    "for alpha in alphas:\n",
    "    for scaler_name, scaler in scalers:\n",
    "        for model_name, model in models:\n",
    "            \n",
    "            y = tmp_df.get([target_column])\n",
    "            X = tmp_df.drop([target_column], axis=1)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=alpha)\n",
    "\n",
    "            pipeline = Pipeline([\n",
    "                ('scaler', scaler),\n",
    "                ('clf', model)\n",
    "            ])\n",
    "\n",
    "            pipeline.fit(X_train, y_train.values.ravel())\n",
    "            if model_name == 'linearregression':\n",
    "                score = cross_val_score(pipeline, X_test, y_test, cv=10, scoring='r2')\n",
    "                score = abs(score.mean())\n",
    "            elif model_name == 'nearest neighbor':\n",
    "                scores = {}\n",
    "                for i in range(1, 6):\n",
    "                    model.n_neighbors = i\n",
    "                    pipeline = Pipeline([\n",
    "                        ('scaler', scaler),\n",
    "                        ('clf', model)\n",
    "                    ])\n",
    "                    \n",
    "                    pipeline.fit(X_train, y_train.values.ravel())\n",
    "                    \n",
    "                    score = pipeline.score(X_test, y_test)\n",
    "                    scores[score] = i\n",
    "                    \n",
    "                max_val = max(scores.keys())\n",
    "                model.n_neighbors = scores[max_val]\n",
    "                model_name += ' {}'.format(i)\n",
    "                scores = max_val\n",
    "            else:\n",
    "                score = pipeline.score(X_test, y_test)\n",
    "\n",
    "            if score > best_val:\n",
    "                best_model['model'] = ((model_name, model))\n",
    "                best_model['scaler'] = ((scaler_name, scaler))\n",
    "                best_model['alpha'] = alpha\n",
    "                best_val = score\n",
    "\n",
    "print('Best Model: {} -> ({},{},{})'.format(\n",
    "    best_val, \n",
    "    best_model['model'][0], \n",
    "    best_model['scaler'][0],\n",
    "    best_model['alpha']\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the amount of refugees for neighboring countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = ml_df[ml_df['Distance'] == 0].copy()\n",
    "\n",
    "scalers = [\n",
    "    ('standard_scaler', StandardScaler())\n",
    "]\n",
    "models = [\n",
    "    ('ridge', RidgeCV()), \n",
    "    ('lasso', LassoCV()), \n",
    "    ('bayesian', BayesianRidge()),\n",
    "    ('nearest neighbor', KNeighborsRegressor(n_neighbors=4)),\n",
    "    ('Random Forest', RandomForestRegressor())\n",
    "]\n",
    "target_column = 'Refugees under UNHCR’s mandate'\n",
    "\n",
    "best_model = {}\n",
    "best_val = 0\n",
    "alphas = [0.2, 0.25, 0.3]\n",
    "\n",
    "for alpha in alphas:\n",
    "    for scaler_name, scaler in scalers:\n",
    "        for model_name, model in models:\n",
    "            \n",
    "            y = tmp_df.get([target_column])\n",
    "            X = tmp_df.drop([target_column], axis=1)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=alpha)\n",
    "\n",
    "            pipeline = Pipeline([\n",
    "                ('scaler', scaler),\n",
    "                ('clf', model)\n",
    "            ])\n",
    "\n",
    "            pipeline.fit(X_train, y_train.values.ravel())\n",
    "            if model_name == 'linearregression':\n",
    "                score = cross_val_score(pipeline, X_test, y_test, cv=10, scoring='r2')\n",
    "                score = abs(score.mean())\n",
    "            elif model_name == 'nearest neighbor':\n",
    "                scores = {}\n",
    "                for i in range(1, 6):\n",
    "                    model.n_neighbors = i\n",
    "                    pipeline = Pipeline([\n",
    "                        ('scaler', scaler),\n",
    "                        ('clf', model)\n",
    "                    ])\n",
    "                    \n",
    "                    pipeline.fit(X_train, y_train.values.ravel())\n",
    "                    \n",
    "                    score = pipeline.score(X_test, y_test)\n",
    "                    scores[score] = i\n",
    "                    \n",
    "                max_val = max(scores.keys())\n",
    "                model.n_neighbors = scores[max_val]\n",
    "                model_name += ' {}'.format(i)\n",
    "                scores = max_val\n",
    "            else:\n",
    "                score = pipeline.score(X_test, y_test)\n",
    "\n",
    "            if score > best_val:\n",
    "                best_model['model'] = ((model_name, model))\n",
    "                best_model['scaler'] = ((scaler_name, scaler))\n",
    "                best_model['alpha'] = alpha\n",
    "                best_val = score\n",
    "\n",
    "print('Best Model: {} -> ({},{},{})'.format(\n",
    "    best_val, \n",
    "    best_model['model'][0], \n",
    "    best_model['scaler'][0],\n",
    "    best_model['alpha']\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "The 3 ML approaches are basic attempts at indicating a certain predictability between a country of origin and the country of asylum. Since the dataset from the dataanalysis part only contains \n",
    "\n",
    "country_of_origin -> distance, refugees, ... -> country_of_asylum\n",
    "\n",
    "conntections, the good prediction results are not surprising.\n",
    "\n",
    "The next 2 steps would be the following:\n",
    "\n",
    "\n",
    "1. Extend the dataset to a country_of_origin -> distance, refugees -> for all country in countries layout\n",
    "\n",
    "This would enable proper prediction results\n",
    "\n",
    "2. Extend the dataset with more attributes about the country of asylum. The reason is that possible countries of asylum which are thousand kilometers away depend on a lot more characteristics than neighboring countries. Best example: What made Syrian refugees travel to Germany?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Were there any difficulties in analysing the data? What were the key insights obtained?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the regard of analyzing data there are many difficulties that make this process really hard. During this exercise we encountered many of these difficulties in real life scenarios. In this exercise we had to deal with data from different datasets, since these datasets weren’t constructed for the same purpose difficulties arise in merging them. After merging these datasets, we ended with a lot of missing values which pointed it out the question, do we have in our dataset meaningful data. Another difficult aspect is to be able to point out relations between different features by using appropriate visualizations techniques. The insights obtained in regard of visualization where that in order to be able to produce visualizations that provide meaning it is best to start by investigating correlation between features this could be done by using correlation matrixes. After visualization the correlation matrix we can start to plot different features by comparing them in visualization. Having plots of the features that are correlated with each other would allow us to investigate different patterns in the data and maybe other hypothesis could be created by just looking at visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which Data Science tools and techniques were learned during this exercise?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this exercise we had the oportunity to learn different techniques in data science such as different visualisation with seaborn library, missing values with K-NN handling, different merging techniques, models and evaluation of the models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
